{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd5ca5d",
   "metadata": {},
   "source": [
    "# FIT Timestamp Conversion Error Analysis\n",
    "\n",
    "This notebook analyzes the issue with timestamp conversion in the Rogue Garmin Bridge application. The error occurs when converting workout data to FIT format.\n",
    "\n",
    "Error from logs:\n",
    "```\n",
    "Error converting bike workout to FIT: timestamp encoded value -629950691 is not in valid range [0, 4294967295]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee007a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to the path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.database import Database\n",
    "from src.data.workout_manager import WorkoutManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14d968",
   "metadata": {},
   "source": [
    "## 1. Connect to the Database and Inspect Workout ID 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "db_path = os.path.join(os.getcwd(), 'src/data/rogue_garmin.db')\n",
    "db = Database(db_path)\n",
    "\n",
    "# Get workout 1\n",
    "workout_id = 1\n",
    "workout = db.get_workout(workout_id)\n",
    "\n",
    "print(f\"Workout ID: {workout['id']}\")\n",
    "print(f\"Type: {workout['workout_type']}\")\n",
    "print(f\"Start time (Unix): {workout['start_time']}\")\n",
    "print(f\"Start time (ISO): {datetime.fromtimestamp(workout['start_time'], tz=timezone.utc).isoformat()}\")\n",
    "if workout.get('end_time'):\n",
    "    print(f\"End time (Unix): {workout['end_time']}\")\n",
    "    print(f\"End time (ISO): {datetime.fromtimestamp(workout['end_time'], tz=timezone.utc).isoformat()}\")\n",
    "    print(f\"Duration: {workout['end_time'] - workout['start_time']} seconds\")\n",
    "else:\n",
    "    print(\"Workout not completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e9a75",
   "metadata": {},
   "source": [
    "## 2. Check Workout Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafe4f4",
   "metadata": {},
   "source": [
    "# FIT Conversion Error Log Analysis\n",
    "\n",
    "This notebook is designed to analyze and process log data for FIT conversion errors in the Rogue Garmin Bridge application. It helps identify patterns, timestamp issues, and common errors to improve the conversion process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924dab51",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import libraries such as re for regular expressions, pandas for data manipulation, and datetime for timestamp processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4271c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a48b7",
   "metadata": {},
   "source": [
    "## Load and Parse Log Data\n",
    "Load the log data into a structured format, such as a pandas DataFrame, by extracting relevant fields like timestamp, log level, module, and message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceafa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_line(line):\n",
    "    \"\"\"Parse a single log line into a structured format.\"\"\"\n",
    "    # Example log format: 2023-05-20 14:30:45,123 - INFO - module_name - Message content\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) - (\\w+) - ([^-]+) - (.+)'\n",
    "    match = re.match(pattern, line)\n",
    "    \n",
    "    if match:\n",
    "        timestamp, level, module, message = match.groups()\n",
    "        return {\n",
    "            'timestamp': timestamp.strip(),\n",
    "            'level': level.strip(),\n",
    "            'module': module.strip(),\n",
    "            'message': message.strip()\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def load_log_file(file_path):\n",
    "    \"\"\"Load and parse a log file into a DataFrame.\"\"\"\n",
    "    log_entries = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                entry = parse_log_line(line)\n",
    "                if entry:\n",
    "                    log_entries.append(entry)\n",
    "                    \n",
    "        return pd.DataFrame(log_entries)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading log file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Replace with your actual log file path\n",
    "log_file_path = \"../logs/rogue_garmin_bridge.log\"\n",
    "\n",
    "# Check if the file exists before loading\n",
    "if os.path.exists(log_file_path):\n",
    "    df_logs = load_log_file(log_file_path)\n",
    "    print(f\"Loaded {len(df_logs)} log entries\")\n",
    "    display(df_logs.head())\n",
    "else:\n",
    "    print(f\"Log file not found: {log_file_path}\")\n",
    "    # Create a sample DataFrame for demonstration\n",
    "    df_logs = pd.DataFrame({\n",
    "        'timestamp': ['2023-05-20 14:30:45,123', '2023-05-20 14:30:46,456'],\n",
    "        'level': ['INFO', 'ERROR'],\n",
    "        'module': ['fit_converter', 'fit_converter'],\n",
    "        'message': ['Processing workout 123456', 'Invalid timestamp range detected: 2023-05-20 14:30:40 - 2023-05-20 14:29:40']\n",
    "    })\n",
    "    print(\"Created sample data for demonstration\")\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "if not df_logs.empty:\n",
    "    df_logs['datetime'] = pd.to_datetime(df_logs['timestamp'], format='%Y-%m-%d %H:%M:%S,%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a28286",
   "metadata": {},
   "source": [
    "## Extract and Analyze Timestamps\n",
    "Convert Unix and FIT timestamps to human-readable formats and calculate differences or inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamps(message):\n",
    "    \"\"\"Extract Unix and FIT timestamps from log messages.\"\"\"\n",
    "    # Example patterns to look for\n",
    "    unix_pattern = r'Unix timestamp: (\\d+)'\n",
    "    fit_pattern = r'FIT timestamp: (\\d+)'\n",
    "    \n",
    "    unix_match = re.search(unix_pattern, message)\n",
    "    fit_match = re.search(fit_pattern, message)\n",
    "    \n",
    "    unix_ts = int(unix_match.group(1)) if unix_match else None\n",
    "    fit_ts = int(fit_match.group(1)) if fit_match else None\n",
    "    \n",
    "    return unix_ts, fit_ts\n",
    "\n",
    "def convert_unix_to_datetime(unix_ts):\n",
    "    \"\"\"Convert Unix timestamp to datetime.\"\"\"\n",
    "    if unix_ts:\n",
    "        return datetime.fromtimestamp(unix_ts)\n",
    "    return None\n",
    "\n",
    "def convert_fit_to_datetime(fit_ts):\n",
    "    \"\"\"Convert FIT timestamp to datetime (seconds since UTC 00:00 Dec 31 1989).\"\"\"\n",
    "    if fit_ts:\n",
    "        fit_epoch = datetime(1989, 12, 31, 0, 0, 0)\n",
    "        return fit_epoch + timedelta(seconds=fit_ts)\n",
    "    return None\n",
    "\n",
    "# Extract timestamps from messages that contain them\n",
    "if not df_logs.empty:\n",
    "    # Create new columns for extracted timestamps\n",
    "    df_logs['unix_ts'], df_logs['fit_ts'] = zip(*df_logs['message'].apply(extract_timestamps))\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    df_logs['unix_datetime'] = df_logs['unix_ts'].apply(convert_unix_to_datetime)\n",
    "    df_logs['fit_datetime'] = df_logs['fit_ts'].apply(convert_fit_to_datetime)\n",
    "    \n",
    "    # Calculate time differences where both timestamps exist\n",
    "    mask = (df_logs['unix_datetime'].notna()) & (df_logs['fit_datetime'].notna())\n",
    "    df_logs.loc[mask, 'timestamp_diff_seconds'] = (\n",
    "        df_logs.loc[mask, 'unix_datetime'] - df_logs.loc[mask, 'fit_datetime']\n",
    "    ).dt.total_seconds()\n",
    "    \n",
    "    # Display timestamp analysis results\n",
    "    timestamp_analysis = df_logs[mask][['datetime', 'unix_datetime', 'fit_datetime', 'timestamp_diff_seconds']]\n",
    "    display(timestamp_analysis.head())\n",
    "    \n",
    "    if not timestamp_analysis.empty:\n",
    "        # Plot histogram of timestamp differences\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        timestamp_analysis['timestamp_diff_seconds'].hist(bins=30)\n",
    "        plt.title('Distribution of Timestamp Differences (Unix - FIT)')\n",
    "        plt.xlabel('Difference in seconds')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb82bc2",
   "metadata": {},
   "source": [
    "## Identify and Handle Errors\n",
    "Filter log entries for errors and warnings, and analyze the causes of issues such as invalid timestamp ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for error and warning messages\n",
    "if not df_logs.empty:\n",
    "    df_errors = df_logs[df_logs['level'] == 'ERROR']\n",
    "    df_warnings = df_logs[df_logs['level'] == 'WARNING']\n",
    "    \n",
    "    print(f\"Found {len(df_errors)} errors and {len(df_warnings)} warnings\")\n",
    "    \n",
    "    # Display the most common errors\n",
    "    if not df_errors.empty:\n",
    "        print(\"\\nMost common error messages:\")\n",
    "        common_errors = df_errors['message'].value_counts().head(10)\n",
    "        display(common_errors)\n",
    "        \n",
    "        # Extract workout IDs from error messages\n",
    "        workout_id_pattern = r'workout[_\\s]?id[:\\s]+(\\w+)'\n",
    "        df_errors['workout_id'] = df_errors['message'].str.extract(workout_id_pattern, expand=False)\n",
    "        \n",
    "        # Count errors by workout ID\n",
    "        if 'workout_id' in df_errors.columns and df_errors['workout_id'].notna().any():\n",
    "            print(\"\\nErrors by workout ID:\")\n",
    "            errors_by_workout = df_errors['workout_id'].value_counts()\n",
    "            display(errors_by_workout.head(10))\n",
    "            \n",
    "    # Analyze invalid timestamp ranges\n",
    "    timestamp_range_pattern = r'Invalid timestamp range detected: (.+)'\n",
    "    timestamp_errors = df_logs[df_logs['message'].str.contains('Invalid timestamp range', na=False)]\n",
    "    \n",
    "    if not timestamp_errors.empty:\n",
    "        print(f\"\\nFound {len(timestamp_errors)} invalid timestamp range errors\")\n",
    "        display(timestamp_errors.head())\n",
    "        \n",
    "        # Extract the timestamp ranges for closer analysis\n",
    "        timestamp_errors['range_info'] = timestamp_errors['message'].str.extract(timestamp_range_pattern, expand=False)\n",
    "        display(timestamp_errors['range_info'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d462e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize different types of errors\n",
    "if not df_logs.empty and 'message' in df_logs.columns:\n",
    "    error_categories = {\n",
    "        'timestamp': ['timestamp', 'time range', 'invalid date'],\n",
    "        'format': ['format', 'malformed', 'parsing error'],\n",
    "        'missing_data': ['missing', 'not found', 'empty'],\n",
    "        'connection': ['connection', 'timeout', 'network'],\n",
    "        'authentication': ['auth', 'token', 'permission'],\n",
    "        'file_io': ['file', 'io error', 'cannot open', 'cannot write']\n",
    "    }\n",
    "    \n",
    "    # Create columns for each error category\n",
    "    for category, keywords in error_categories.items():\n",
    "        pattern = '|'.join(keywords)\n",
    "        df_logs[f'{category}_error'] = df_logs['message'].str.contains(\n",
    "            pattern, case=False, na=False\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Sum up each category\n",
    "    error_counts = df_logs[[f'{category}_error' for category in error_categories.keys()]].sum()\n",
    "    \n",
    "    # Plot error categories\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    error_counts.plot(kind='bar', color='crimson')\n",
    "    plt.title('Error Categories')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a61050",
   "metadata": {},
   "source": [
    "## Generate Summary Report\n",
    "Summarize the findings, including the number of errors, affected workouts, and any patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary report of the log analysis\n",
    "def generate_summary_report(df):\n",
    "    \"\"\"Generate a summary report of log analysis findings.\"\"\"\n",
    "    if df.empty or 'level' not in df.columns:\n",
    "        return \"No valid log data to analyze.\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_entries = len(df)\n",
    "    log_timespan = None\n",
    "    if 'datetime' in df.columns and df['datetime'].notna().any():\n",
    "        min_time = df['datetime'].min()\n",
    "        max_time = df['datetime'].max()\n",
    "        log_timespan = f\"{min_time} to {max_time}\"\n",
    "    \n",
    "    # Count by log level\n",
    "    level_counts = df['level'].value_counts()\n",
    "    \n",
    "    # Count by module\n",
    "    module_counts = df['module'].value_counts() if 'module' in df.columns else None\n",
    "    \n",
    "    # Error analysis\n",
    "    error_df = df[df['level'] == 'ERROR'] if 'level' in df.columns else pd.DataFrame()\n",
    "    total_errors = len(error_df)\n",
    "    \n",
    "    # Affected workouts\n",
    "    affected_workouts = None\n",
    "    if 'workout_id' in error_df.columns and error_df['workout_id'].notna().any():\n",
    "        affected_workouts = error_df['workout_id'].nunique()\n",
    "    \n",
    "    # Timestamp error analysis\n",
    "    timestamp_errors = df[df['message'].str.contains('timestamp|time range', case=False, na=False)]\n",
    "    \n",
    "    # Generate the report\n",
    "    report = f\"\"\"\n",
    "    # Log Analysis Summary Report\n",
    "    \n",
    "    ## Overview\n",
    "    - Total log entries: {total_entries}\n",
    "    - Log time span: {log_timespan if log_timespan else 'Unknown'}\n",
    "    - Total errors: {total_errors}\n",
    "    - Affected workouts: {affected_workouts if affected_workouts is not None else 'Unknown'}\n",
    "    \n",
    "    ## Log Level Distribution\n",
    "    {level_counts.to_string() if level_counts is not None else 'No data'}\n",
    "    \n",
    "    ## Module Distribution\n",
    "    {module_counts.head(10).to_string() if module_counts is not None else 'No data'}\n",
    "    \n",
    "    ## Error Analysis\n",
    "    - Timestamp-related errors: {len(timestamp_errors)}\n",
    "    - Most common error: {error_df['message'].value_counts().index[0] if not error_df.empty and 'message' in error_df.columns else 'None'}\n",
    "    \n",
    "    ## Recommendations\n",
    "    1. Review timestamp handling in the FIT conversion process\n",
    "    2. Add more validation checks for workout data before conversion\n",
    "    3. Implement better error recovery mechanisms\n",
    "    4. Consider adding more detailed logging for problematic areas\n",
    "    \"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and display the summary report\n",
    "if not df_logs.empty:\n",
    "    summary_report = generate_summary_report(df_logs)\n",
    "    print(summary_report)\n",
    "    \n",
    "    # Save the report to a markdown file\n",
    "    report_path = \"log_analysis_report.md\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(summary_report)\n",
    "    print(f\"Report saved to {report_path}\")\n",
    "    \n",
    "    # Create visualizations for the report\n",
    "    if 'level' in df_logs.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        df_logs['level'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Log Entries by Level')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "    \n",
    "    # Timeline of errors\n",
    "    if 'datetime' in df_logs.columns and 'level' in df_logs.columns:\n",
    "        errors_over_time = df_logs[df_logs['level'] == 'ERROR'].set_index('datetime')\n",
    "        if not errors_over_time.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            errors_over_time.resample('1H').size().plot()\n",
    "            plt.title('Errors Over Time')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Number of Errors')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84ce81",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive analysis of FIT conversion error logs. The insights gained from this analysis can help identify:\n",
    "\n",
    "1. Common error patterns in the FIT conversion process\n",
    "2. Issues with timestamp handling and validation\n",
    "3. Specific workouts that consistently cause errors\n",
    "4. Trends in error occurrence over time\n",
    "\n",
    "Using these insights, the development team can make targeted improvements to the Rogue Garmin Bridge application to increase reliability and reduce conversion errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
