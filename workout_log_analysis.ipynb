{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe64151",
   "metadata": {},
   "source": [
    "# Workout Log Analysis Notebook\n",
    "\n",
    "This notebook analyzes and processes log data from a simulated workout session to extract meaningful insights and visualize workout metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eb227",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries such as pandas, matplotlib, and re for data processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298fc50",
   "metadata": {},
   "source": [
    "## Load and Parse Log Data\n",
    "\n",
    "Load the log data into a structured format (e.g., DataFrame) by parsing timestamps, log levels, and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d525138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(log_file_path):\n",
    "    \"\"\"\n",
    "    Parse a log file and return a structured DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        log_file_path (str): Path to the log file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with parsed log entries\n",
    "    \"\"\"\n",
    "    # Log pattern: timestamp - level - component - message\n",
    "    log_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) - (\\w+) - (\\w+) - (.+)'\n",
    "    \n",
    "    log_entries = []\n",
    "    \n",
    "    try:\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                match = re.match(log_pattern, line)\n",
    "                if match:\n",
    "                    timestamp_str, level, component, message = match.groups()\n",
    "                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')\n",
    "                    log_entries.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'level': level,\n",
    "                        'component': component,\n",
    "                        'message': message.strip()\n",
    "                    })\n",
    "                else:\n",
    "                    # This might be a continuation of a previous message\n",
    "                    if log_entries:\n",
    "                        log_entries[-1]['message'] += '\\n' + line.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing log file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(log_entries)\n",
    "\n",
    "# Example usage:\n",
    "# Specify the path to your log file\n",
    "log_file_path = input(\"Enter the path to the log file: \")\n",
    "\n",
    "if os.path.exists(log_file_path):\n",
    "    log_df = parse_log_file(log_file_path)\n",
    "    if not log_df.empty:\n",
    "        print(f\"Successfully parsed {len(log_df)} log entries\")\n",
    "        display(log_df.head())\n",
    "    else:\n",
    "        print(\"No log entries were parsed from the file\")\n",
    "else:\n",
    "    print(f\"Log file not found at: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea280e9",
   "metadata": {},
   "source": [
    "## Filter Simulator Data\n",
    "\n",
    "Extract simulator-related logs to analyze the data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simulator_logs(log_df):\n",
    "    \"\"\"\n",
    "    Extract simulator-related logs from the parsed log DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        log_df (pd.DataFrame): DataFrame with parsed log entries\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with only simulator-related logs\n",
    "    \"\"\"\n",
    "    if log_df.empty:\n",
    "        print(\"No log data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter logs related to simulator\n",
    "    simulator_df = log_df[\n",
    "        log_df['component'].str.contains('simulator', case=False) | \n",
    "        log_df['message'].str.contains('simulator|simulated', case=False)\n",
    "    ].copy()\n",
    "    \n",
    "    if simulator_df.empty:\n",
    "        print(\"No simulator-related logs found\")\n",
    "    else:\n",
    "        print(f\"Found {len(simulator_df)} simulator-related log entries\")\n",
    "    \n",
    "    return simulator_df\n",
    "\n",
    "if 'log_df' in locals() and not log_df.empty:\n",
    "    simulator_logs = extract_simulator_logs(log_df)\n",
    "    if not simulator_logs.empty:\n",
    "        display(simulator_logs.head())\n",
    "else:\n",
    "    print(\"Please parse a log file first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a250029",
   "metadata": {},
   "source": [
    "## Extract Workout Data from Logs\n",
    "\n",
    "Parse the log entries to extract workout data points and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19796a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_points(log_df):\n",
    "    \"\"\"\n",
    "    Extract workout data points from log entries.\n",
    "    \n",
    "    Args:\n",
    "        log_df (pd.DataFrame): DataFrame with parsed log entries\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with structured workout data points\n",
    "    \"\"\"\n",
    "    data_points = []\n",
    "    \n",
    "    # Look for log entries containing JSON data\n",
    "    data_pattern = r\"Received data: (.+)\"\n",
    "    \n",
    "    for _, row in log_df.iterrows():\n",
    "        match = re.search(data_pattern, row['message'])\n",
    "        if match:\n",
    "            json_str = match.group(1).strip()\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                # Add timestamp from log\n",
    "                data['log_timestamp'] = row['timestamp']\n",
    "                data_points.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                # Not valid JSON, might be a different format\n",
    "                continue\n",
    "    \n",
    "    if not data_points:\n",
    "        print(\"No data points found in logs\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    data_df = pd.DataFrame(data_points)\n",
    "    print(f\"Extracted {len(data_df)} data points from logs\")\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "if 'simulator_logs' in locals() and not simulator_logs.empty:\n",
    "    data_points_df = extract_data_points(simulator_logs)\n",
    "    if not data_points_df.empty:\n",
    "        print(\"\\nColumns in data points DataFrame:\")\n",
    "        print(data_points_df.columns.tolist())\n",
    "        display(data_points_df.head())\n",
    "else:\n",
    "    print(\"Please extract simulator logs first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d3a90",
   "metadata": {},
   "source": [
    "## Analyze Data Generation Frequency\n",
    "\n",
    "Analyze how frequently data points are generated during the workout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55925b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_frequency(data_df):\n",
    "    \"\"\"\n",
    "    Analyze the frequency of data point generation.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): DataFrame with extracted data points\n",
    "    \"\"\"\n",
    "    if data_df.empty or 'log_timestamp' not in data_df.columns:\n",
    "        print(\"No timestamp data available for frequency analysis\")\n",
    "        return\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    data_df = data_df.sort_values('log_timestamp')\n",
    "    \n",
    "    # Calculate time differences between consecutive data points\n",
    "    data_df['time_diff'] = data_df['log_timestamp'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Summary statistics for time differences\n",
    "    time_diffs = data_df['time_diff'].dropna()\n",
    "    \n",
    "    if time_diffs.empty:\n",
    "        print(\"Not enough data points to analyze frequency\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nData Generation Frequency Statistics (seconds):\")\n",
    "    print(f\"Min interval: {time_diffs.min():.4f}\")\n",
    "    print(f\"Max interval: {time_diffs.max():.4f}\")\n",
    "    print(f\"Mean interval: {time_diffs.mean():.4f}\")\n",
    "    print(f\"Median interval: {time_diffs.median():.4f}\")\n",
    "    \n",
    "    # Plot frequency histogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(time_diffs, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Time Between Data Points (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Data Generation Frequency Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axvline(time_diffs.mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.text(time_diffs.mean()*1.1, plt.ylim()[1]*0.9, f'Mean: {time_diffs.mean():.2f}s', color='red')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot time intervals over time to see if there's any pattern\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(data_df['log_timestamp'][1:], time_diffs, '-o', alpha=0.5, markersize=3)\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Time Interval (seconds)')\n",
    "    plt.title('Data Generation Intervals Over Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(1.0, color='green', linestyle='dashed', linewidth=1)\n",
    "    plt.text(plt.xlim()[0], 1.1, 'Target: 1.0s', color='green')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'data_points_df' in locals() and not data_points_df.empty and 'log_timestamp' in data_points_df.columns:\n",
    "    analyze_data_frequency(data_points_df)\n",
    "else:\n",
    "    print(\"Please extract data points with timestamps first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbd28",
   "metadata": {},
   "source": [
    "## Visualize Workout Metrics\n",
    "\n",
    "Create visualizations for key workout metrics extracted from logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(data_df):\n",
    "    \"\"\"\n",
    "    Visualize key workout metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): DataFrame with extracted data points\n",
    "    \"\"\"\n",
    "    if data_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Identify numeric columns for potential metrics\n",
    "    numeric_cols = data_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Common workout metrics to visualize if available\n",
    "    metric_names = [\n",
    "        'power', 'instantaneous_power',\n",
    "        'cadence', 'instantaneous_cadence',\n",
    "        'heart_rate',\n",
    "        'speed', 'instantaneous_speed',\n",
    "        'distance', 'total_distance',\n",
    "        'calories', 'total_calories'\n",
    "    ]\n",
    "    \n",
    "    # Filter for available metrics\n",
    "    available_metrics = [col for col in metric_names if col in numeric_cols]\n",
    "    \n",
    "    if not available_metrics:\n",
    "        print(\"No recognized workout metrics found in the data\")\n",
    "        print(\"Available numeric columns:\", numeric_cols)\n",
    "        return\n",
    "    \n",
    "    print(f\"Visualizing {len(available_metrics)} workout metrics: {', '.join(available_metrics)}\")\n",
    "    \n",
    "    # Sort by timestamp if available\n",
    "    if 'log_timestamp' in data_df.columns:\n",
    "        data_df = data_df.sort_values('log_timestamp')\n",
    "        x_values = data_df['log_timestamp']\n",
    "        x_label = 'Time'\n",
    "    else:\n",
    "        # Use row index as x-axis\n",
    "        x_values = data_df.index\n",
    "        x_label = 'Data Point Index'\n",
    "    \n",
    "    # Create a multi-panel figure for each metric\n",
    "    fig, axes = plt.subplots(len(available_metrics), 1, figsize=(12, 4*len(available_metrics)), sharex=True)\n",
    "    \n",
    "    # Handle case with only one metric\n",
    "    if len(available_metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics):\n",
    "        ax = axes[i]\n",
    "        ax.plot(x_values, data_df[metric], '-o', alpha=0.7, markersize=3)\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} Over Time')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add some statistics\n",
    "        if not data_df[metric].empty:\n",
    "            mean_val = data_df[metric].mean()\n",
    "            max_val = data_df[metric].max()\n",
    "            ax.axhline(mean_val, color='red', linestyle='dashed', linewidth=1)\n",
    "            ax.text(x_values.iloc[0], mean_val*1.1, f'Mean: {mean_val:.1f}', color='red')\n",
    "            ax.text(x_values.iloc[0], max_val*0.9, f'Max: {max_val:.1f}', color='blue')\n",
    "    \n",
    "    # Set common x-axis label\n",
    "    axes[-1].set_xlabel(x_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each metric\n",
    "    print(\"\\nSummary Statistics for Workout Metrics:\")\n",
    "    display(data_df[available_metrics].describe())\n",
    "\n",
    "if 'data_points_df' in locals() and not data_points_df.empty:\n",
    "    visualize_metrics(data_points_df)\n",
    "else:\n",
    "    print(\"Please extract data points first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dda2e5",
   "metadata": {},
   "source": [
    "## Detect Workout Start and End\n",
    "\n",
    "Identify the start and end of workout sessions from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_workout_sessions(log_df):\n",
    "    \"\"\"\n",
    "    Detect workout start and end events in the logs.\n",
    "    \n",
    "    Args:\n",
    "        log_df (pd.DataFrame): DataFrame with parsed log entries\n",
    "    \"\"\"\n",
    "    if log_df.empty:\n",
    "        print(\"No log data available\")\n",
    "        return\n",
    "    \n",
    "    # Keywords for workout start and end\n",
    "    start_patterns = [\n",
    "        r\"workout started\",\n",
    "        r\"starting workout\",\n",
    "        r\"begin(ning)? workout\",\n",
    "        r\"workout session (started|beginning)\",\n",
    "        r\"new workout (created|initiated)\"\n",
    "    ]\n",
    "    \n",
    "    end_patterns = [\n",
    "        r\"workout (ended|finished|completed)\",\n",
    "        r\"ending workout\",\n",
    "        r\"stopping workout\",\n",
    "        r\"workout session (ended|finished|completed)\",\n",
    "        r\"workout (saved|recorded)\"\n",
    "    ]\n",
    "    \n",
    "    # Find start and end events\n",
    "    start_events = []\n",
    "    end_events = []\n",
    "    \n",
    "    for _, row in log_df.iterrows():\n",
    "        message = row['message'].lower()\n",
    "        \n",
    "        # Check for start patterns\n",
    "        if any(re.search(pattern, message) for pattern in start_patterns):\n",
    "            start_events.append({\n",
    "                'timestamp': row['timestamp'],\n",
    "                'message': row['message']\n",
    "            })\n",
    "        \n",
    "        # Check for end patterns\n",
    "        if any(re.search(pattern, message) for pattern in end_patterns):\n",
    "            end_events.append({\n",
    "                'timestamp': row['timestamp'],\n",
    "                'message': row['message']\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(start_events)} workout start events and {len(end_events)} end events\")\n",
    "    \n",
    "    # Display start events\n",
    "    if start_events:\n",
    "        print(\"\\nWorkout Start Events:\")\n",
    "        start_df = pd.DataFrame(start_events)\n",
    "        display(start_df)\n",
    "    \n",
    "    # Display end events\n",
    "    if end_events:\n",
    "        print(\"\\nWorkout End Events:\")\n",
    "        end_df = pd.DataFrame(end_events)\n",
    "        display(end_df)\n",
    "    \n",
    "    # Calculate workout durations if we have matching start/end pairs\n",
    "    if start_events and end_events:\n",
    "        print(\"\\nWorkout Sessions:\")\n",
    "        sessions = []\n",
    "        \n",
    "        # Simple matching algorithm - may need refinement for complex logs\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        \n",
    "        while start_idx < len(start_events) and end_idx < len(end_events):\n",
    "            start_time = start_events[start_idx]['timestamp']\n",
    "            end_time = end_events[end_idx]['timestamp']\n",
    "            \n",
    "            if end_time > start_time:  # Valid session\n",
    "                duration = (end_time - start_time).total_seconds()\n",
    "                sessions.append({\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'duration_seconds': duration,\n",
    "                    'duration_formatted': f\"{int(duration//60)}:{int(duration%60):02d}\"\n",
    "                })\n",
    "                start_idx += 1\n",
    "                end_idx += 1\n",
    "            else:  # End event without matching start\n",
    "                end_idx += 1\n",
    "        \n",
    "        if sessions:\n",
    "            sessions_df = pd.DataFrame(sessions)\n",
    "            display(sessions_df)\n",
    "        else:\n",
    "            print(\"No clear workout sessions identified\")\n",
    "\n",
    "if 'log_df' in locals() and not log_df.empty:\n",
    "    detect_workout_sessions(log_df)\n",
    "else:\n",
    "    print(\"Please parse a log file first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3cfcf2",
   "metadata": {},
   "source": [
    "## Analyze Error and Warning Logs\n",
    "\n",
    "Review error and warning messages to identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de87b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors_and_warnings(log_df):\n",
    "    \"\"\"\n",
    "    Analyze error and warning messages in the logs.\n",
    "    \n",
    "    Args:\n",
    "        log_df (pd.DataFrame): DataFrame with parsed log entries\n",
    "    \"\"\"\n",
    "    if log_df.empty:\n",
    "        print(\"No log data available\")\n",
    "        return\n",
    "    \n",
    "    # Filter for errors and warnings\n",
    "    error_logs = log_df[log_df['level'] == 'ERROR']\n",
    "    warning_logs = log_df[log_df['level'] == 'WARNING']\n",
    "    \n",
    "    print(f\"Found {len(error_logs)} ERROR logs and {len(warning_logs)} WARNING logs\")\n",
    "    \n",
    "    # Display errors\n",
    "    if not error_logs.empty:\n",
    "        print(\"\\nERROR Messages:\")\n",
    "        display(error_logs[['timestamp', 'component', 'message']])\n",
    "    \n",
    "    # Display warnings\n",
    "    if not warning_logs.empty:\n",
    "        print(\"\\nWARNING Messages:\")\n",
    "        display(warning_logs[['timestamp', 'component', 'message']])\n",
    "    \n",
    "    # Count errors by component\n",
    "    if not error_logs.empty:\n",
    "        print(\"\\nErrors by Component:\")\n",
    "        error_counts = error_logs['component'].value_counts()\n",
    "        display(error_counts)\n",
    "        \n",
    "        # Plot error distribution by component\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        error_counts.plot(kind='bar')\n",
    "        plt.title('Error Count by Component')\n",
    "        plt.xlabel('Component')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if 'log_df' in locals() and not log_df.empty:\n",
    "    analyze_errors_and_warnings(log_df)\n",
    "else:\n",
    "    print(\"Please parse a log file first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51796cea",
   "metadata": {},
   "source": [
    "## Compare with Database\n",
    "\n",
    "Compare log data with what's stored in the database to verify data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde66f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_database():\n",
    "    \"\"\"\n",
    "    Connect to the database and compare log data with database records.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import sqlite3\n",
    "        \n",
    "        # Locate the database - could be in various locations\n",
    "        possible_db_paths = [\n",
    "            os.path.join(os.getcwd(), 'rogue_garmin.db'),\n",
    "            os.path.join(os.getcwd(), '..', 'src', 'data', 'rogue_garmin.db'),\n",
    "            os.path.join('E:\\\\', 'rogue_garmin_bridge', 'src', 'data', 'rogue_garmin.db')\n",
    "        ]\n",
    "        \n",
    "        db_path = None\n",
    "        for path in possible_db_paths:\n",
    "            if os.path.exists(path):\n",
    "                db_path = path\n",
    "                break\n",
    "        \n",
    "        if not db_path:\n",
    "            print(\"Database not found. Please specify the correct path.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Connecting to database at: {db_path}\")\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Get recent workouts\n",
    "        workouts_df = pd.read_sql_query(\n",
    "            \"SELECT * FROM workouts ORDER BY id DESC LIMIT 5\",\n",
    "            conn\n",
    "        )\n",
    "        \n",
    "        if workouts_df.empty:\n",
    "            print(\"No workouts found in the database\")\n",
    "            conn.close()\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(workouts_df)} recent workouts in the database\")\n",
    "        display(workouts_df)\n",
    "        \n",
    "        # Get the most recent workout\n",
    "        most_recent_workout_id = workouts_df.iloc[0]['id']\n",
    "        \n",
    "        # Get data points for this workout\n",
    "        data_points_db = pd.read_sql_query(\n",
    "            f\"SELECT * FROM workout_data WHERE workout_id = {most_recent_workout_id} ORDER BY timestamp\",\n",
    "            conn\n",
    "        )\n",
    "        \n",
    "        print(f\"Found {len(data_points_db)} data points for workout ID {most_recent_workout_id}\")\n",
    "        \n",
    "        if not data_points_db.empty:\n",
    "            display(data_points_db.head())\n",
    "            \n",
    "            # Compare with log data if available\n",
    "            if 'data_points_df' in globals() and not data_points_df.empty:\n",
    "                print(\"\\nComparison between log data and database data:\")\n",
    "                print(f\"Log data points: {len(data_points_df)}\")\n",
    "                print(f\"Database data points: {len(data_points_db)}\")\n",
    "                \n",
    "                # More detailed comparison could be added here\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing with database: {e}\")\n",
    "\n",
    "# Uncomment to run the database comparison\n",
    "# compare_with_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dc69c",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "Summarize findings and provide recommendations based on the log analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376372b0",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "Based on the log analysis, here are the key findings:\n",
    "\n",
    "1. **Data Generation Frequency**: The simulator appears to be generating data points at regular intervals (ideally every second)\n",
    "2. **Metrics Coverage**: The data includes essential workout metrics like power, cadence, distance, and calories\n",
    "3. **Workout Sessions**: The logs show clear workout start and end events\n",
    "4. **Error Analysis**: Any errors or warnings in the logs should be addressed\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Data Consistency**: Ensure that timestamps are unique to prevent database collisions\n",
    "2. **Error Handling**: Address any recurring errors or warnings in the logs\n",
    "3. **Data Flow**: Verify that all data points from the simulator are correctly stored in the database\n",
    "4. **Performance**: Monitor the time between data generation and storage to ensure real-time processing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
